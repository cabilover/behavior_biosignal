{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pickle\n",
    "import joblib\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import pingouin as pg\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import shap\n",
    "\n",
    "from pprint import pprint\n",
    "from shutil import copyfile\n",
    "from sklearn import metrics, dummy, linear_model, ensemble, svm, neural_network\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.preprocessing import StandardScaler, LabelEncoder, OneHotEncoder\n",
    "from sklearn.model_selection import GridSearchCV, train_test_split\n",
    "\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import roc_curve\n",
    "from sklearn.feature_selection import SelectKBest, f_classif\n",
    "\n",
    "from sklearn.datasets import load_iris"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "config_path = os.path.abspath(r'C:\\Users\\USER\\Guro_Psy_KJH Dropbox\\1.Projects\\1_anxiety_VR\\3_Data\\5_prediction_model\\1_2023_report')\n",
    "data = 'vrabes_preprocessed.pkl'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def main(config_path,data):\n",
    "\n",
    "    # prepare dataset\n",
    "    with open(data,'rb') as f:\n",
    "        pre = pickle.load(f)\n",
    "    \n",
    "    ses_01_as = pre['ses-01_as']\n",
    "    ses_01_demo = pre['ses-01_demo']\n",
    "    ses_01_crf = pre['ses-01_crf']\n",
    "    ses_01_hrv = pre['ses-01_hrv']\n",
    "\n",
    "    ses_01 = pd.concat([ses_01_demo,ses_01_as,ses_01_hrv], axis=1)\n",
    "    \n",
    "    # prepare result path\n",
    "    if not os.path.isdir(os.path.join(config_path,'reulst_dir')):\n",
    "        os.mkdir(os.path.join(config_path,'reulst_dir'))\n",
    "    os.mkdir(os.path.join(config_path,'reulst_dir', 'fig'))\n",
    "    os.mkdir(os.path.join(config_path,'reulst_dir', 'fig', 'confusion_matrix'))\n",
    "    os.mkdir(os.path.join(config_path,'reulst_dir', 'fig', 'roc'))\n",
    "    os.mkdir(os.path.join(config_path,'reulst_dir', 'model'))\n",
    "    os.mkdir(os.path.join(config_path,'reulst_dir', 'result'))\n",
    "    os.mkdir(os.path.join(config_path,'reulst_dir', 'result', 'cv'))\n",
    "\n",
    "    ### start analysis\n",
    "\n",
    "    # define models\n",
    "    models = [\n",
    "        ('Dummy', dummy.DummyClassifier()),\n",
    "        ('LogReg', linear_model.LogisticRegression()),\n",
    "        ('SVM', svm.SVC()),\n",
    "        ('RandomForest', ensemble.RandomForestClassifier()),\n",
    "        ('MLP', neural_network.MLPClassifier())\n",
    "    ]\n",
    "\n",
    "    # define hyperparameter search space\n",
    "    param_grid = {\n",
    "        'Dummy': {\n",
    "        \n",
    "        },\n",
    "        'LogReg': {\n",
    "            'solver': ['newton-cg', 'lbfgs', 'liblinear', 'saga'],\n",
    "            'penalty': ['l1', 'l2', 'elasticnet'],\n",
    "            'C': [float(10**x) for x in range(-4, 4)],\n",
    "            'l1_ratio': np.random.uniform(size=5),\n",
    "            'max_iter': [10000]\n",
    "        },\n",
    "        'SVM': {\n",
    "            'kernel': ['linear', 'rbf', 'poly'],\n",
    "            'C': [float(10**x) for x in range(-3, 3)],\n",
    "            'gamma': [float(10**x) for x in range(-3, 4)],\n",
    "            'probability': [True],\n",
    "        },\n",
    "        'RandomForest': {\n",
    "            'n_estimators': [10, 25, 100],            \n",
    "            'max_depth': [int(x) for x in np.linspace(10, 110, num = 11)]+[None],\n",
    "            'min_samples_split': [2, 5, 10],\n",
    "            'min_samples_leaf': [1, 2, 4],\n",
    "            'bootstrap': [True, False]\n",
    "        },\n",
    "        'MLP': {\n",
    "            'hidden_layer_sizes': [10, 50, 100],\n",
    "            'solver': ['lbfgs', 'sgd', 'adam'],\n",
    "            'alpha': [float(10**x) for x in range(-2,2)],\n",
    "            'batch_size': [int(2**x) for x in range(3, 5)],\n",
    "            'learning_rate': ['constant', 'invscaling', 'adaptive'],\n",
    "            'max_iter': [1000],\n",
    "            'warm_start': [False, True]\n",
    "        },\n",
    "        \n",
    "    }\n",
    "\n",
    "    # prepare input-label data pairs\n",
    "    \n",
    "    X_pre = ses_01.drop(['ses-01_group'],axis =1).to_numpy()\n",
    "    y = ses_01['ses-01_group'].to_numpy()\n",
    "\n",
    "    # # feature selection\n",
    "    feat_sel = SelectKBest(f_classif, k=5)\n",
    "    X = feat_sel.fit_transform(X_pre,y)\n",
    "    selected_feature = {0:feat_sel.get_feature_names_out()}    \n",
    "    pd.DataFrame.from_dict(selected_feature).to_csv(os.path.join(config_path,'reulst_dir', 'result', 'selected_features.csv'))\n",
    "    \n",
    "\n",
    "    # split data pairs into train and test datasets\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X,y,test_size=0.25, random_state=123, shuffle=True, stratify=y)\n",
    "\n",
    "    # standardize input data\n",
    "    scaler = StandardScaler()\n",
    "    scaler.fit(X_train)\n",
    "    X_train = scaler.transform(X_train)\n",
    "    X_test = scaler.transform(X_test)\n",
    "\n",
    "    # plot demographics\n",
    "    sns.set_theme(context='paper', style='white', font_scale=1.5,palette='muted')\n",
    "\n",
    "    df_demo = ses_01_demo.drop(['ses-01_group'],axis=1).iloc[:,0:2]\n",
    "    df_crf = ses_01_crf[['ses-01_PDSS_SUM', 'ses-01_GAD_SUM', 'ses-01_HADS_d']]\n",
    "    df_demo = df_demo.join(df_crf)\n",
    "    df_demo.columns = ['Age', 'Sex', 'PDSS', 'GAD','HADS-Depression']\n",
    "    fig, ax = plt.subplots(ncols=5, figsize=(15,3.5),sharey=True)\n",
    "\n",
    "    for i, x in enumerate(['Age', 'Sex', 'PDSS', 'GAD', 'HADS-Depression']):\n",
    "        sns.histplot(x=x, data=df_demo, ax=ax[i], kde=True)\n",
    "        if x=='PDSS':\n",
    "            ax[i].axvline(x=df_demo[x].median(), color='r', linestyle='--', linewidth=2.0)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.savefig(os.path.join(config_path,'reulst_dir', 'fig','demographics.png'))\n",
    "    plt.close()\n",
    "\n",
    "    # start model training\n",
    "    result_dict = {}\n",
    "    weight_importance_dict = {}    \n",
    "\n",
    "    for model_name, model in models:\n",
    "        # train 5-fold cross validation with exhaustive grid search\n",
    "        model_hyperparamsearch = GridSearchCV(estimator=model, param_grid=param_grid[model_name], cv=5, verbose=0, n_jobs=-1)\n",
    "        model_hyperparamsearch.fit(X_train, y_train)\n",
    "        best_estimator = model_hyperparamsearch.best_estimator_\n",
    "\n",
    "        # get prediction on the test dataset\n",
    "        pred_test = model_hyperparamsearch.predict(X_test)\n",
    "\n",
    "        # keept test performance results\n",
    "        result_dict[model_name] = {}\n",
    "        result_dict[model_name]['uncertainty_mean'] = best_estimator.predict_proba(X_test).max(1).mean()\n",
    "        result_dict[model_name]['uncertainty_std'] = best_estimator.predict_proba(X_test).max(1).std()\n",
    "        result_dict[model_name]['test_acc'] = metrics.accuracy_score(y_test, pred_test)\n",
    "        result_dict[model_name]['test_acc_bal'] = metrics.balanced_accuracy_score(y_test, pred_test)\n",
    "        result_dict[model_name]['test_f1'] = metrics.f1_score(y_test, pred_test, average='binary')\n",
    "        result_dict[model_name]['test_cohen_kappa'] = metrics.cohen_kappa_score(y_test, pred_test, labels=y)\n",
    "        result_dict[model_name]['val'] = model_hyperparamsearch.best_score_\n",
    "        pprint(result_dict)\n",
    "\n",
    "        # plot confusion matrix\n",
    "        cm = metrics.confusion_matrix(y_test, pred_test)\n",
    "        cm = cm/float(cm.sum()) # convert to ratio       \n",
    "        ax = sns.heatmap(cm, annot=True, xticklabels=['Normal','Panic'], yticklabels=['Normal','Panic'], cmap='Blues', fmt='.2f', square=True)\n",
    "        plt.suptitle(f'{model_name}')\n",
    "        plt.tight_layout()\n",
    "        plt.savefig(os.path.join(config_path,'reulst_dir', 'fig', 'confusion_matrix', model_name+'.png'))\n",
    "        plt.close()\n",
    "\n",
    "        # plot ROC curve\n",
    "        fpr, tpr, thresholds = roc_curve(y_test,pred_test)\n",
    "        plt.plot(fpr, tpr, 'o-', label=\"Decision Tree\")\n",
    "        plt.plot([0, 1], [0, 1], 'k--', label=\"random guess\")\n",
    "        plt.xlabel('Fall-Out')\n",
    "        plt.ylabel('Recall')\n",
    "        plt.title('Receiver operating characteristic example')\n",
    "        plt.savefig(os.path.join(config_path,'reulst_dir', 'fig', 'roc', model_name+'.png'))\n",
    "        plt.close()\n",
    "\n",
    "        # save model and results\n",
    "        joblib.dump(best_estimator, os.path.join(config_path,'reulst_dir', 'model', model_name+'.joblib'))\n",
    "        pd.DataFrame.from_dict(model_hyperparamsearch.cv_results_).to_csv(os.path.join(config_path,'reulst_dir', 'result', 'cv', model_name+'.csv'))\n",
    "        pd.DataFrame.from_dict(result_dict).T.to_csv(os.path.join(config_path,'reulst_dir', 'result', 'result.csv'))\n",
    "        pd.DataFrame.from_dict(weight_importance_dict).T.to_csv(os.path.join(config_path,'reulst_dir', 'result', 'importance_weight.csv'))  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Features [ 25  26  27  43  61  62  63  79  97  98  99 115 133 134 135 151 169 170\n",
      " 171 187 205 206 207 223 241 242 243 259 277 278 279 295 313 314 315 331\n",
      " 349 350 351 367] are constant.\n",
      "invalid value encountered in divide\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'Dummy': {'test_acc': 0.5,\n",
      "           'test_acc_bal': 0.5,\n",
      "           'test_cohen_kappa': 0.0,\n",
      "           'test_f1': 0.6666666666666666,\n",
      "           'uncertainty_mean': 0.5111111111111111,\n",
      "           'uncertainty_std': 0.0,\n",
      "           'val': 0.4444444444444445}}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "1000 fits failed out of a total of 2400.\n",
      "The score on these train-test partitions for these parameters will be set to nan.\n",
      "If these failures are not expected, you can try to debug them by setting error_score='raise'.\n",
      "\n",
      "Below are more details about the failures:\n",
      "--------------------------------------------------------------------------------\n",
      "200 fits failed with the following error:\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\USER\\anaconda3\\envs\\main\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 686, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"c:\\Users\\USER\\anaconda3\\envs\\main\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 1162, in fit\n",
      "    solver = _check_solver(self.solver, self.penalty, self.dual)\n",
      "  File \"c:\\Users\\USER\\anaconda3\\envs\\main\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 54, in _check_solver\n",
      "    raise ValueError(\n",
      "ValueError: Solver newton-cg supports only 'l2' or 'none' penalties, got l1 penalty.\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "200 fits failed with the following error:\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\USER\\anaconda3\\envs\\main\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 686, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"c:\\Users\\USER\\anaconda3\\envs\\main\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 1162, in fit\n",
      "    solver = _check_solver(self.solver, self.penalty, self.dual)\n",
      "  File \"c:\\Users\\USER\\anaconda3\\envs\\main\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 54, in _check_solver\n",
      "    raise ValueError(\n",
      "ValueError: Solver lbfgs supports only 'l2' or 'none' penalties, got l1 penalty.\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "200 fits failed with the following error:\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\USER\\anaconda3\\envs\\main\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 686, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"c:\\Users\\USER\\anaconda3\\envs\\main\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 1162, in fit\n",
      "    solver = _check_solver(self.solver, self.penalty, self.dual)\n",
      "  File \"c:\\Users\\USER\\anaconda3\\envs\\main\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 54, in _check_solver\n",
      "    raise ValueError(\n",
      "ValueError: Solver newton-cg supports only 'l2' or 'none' penalties, got elasticnet penalty.\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "200 fits failed with the following error:\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\USER\\anaconda3\\envs\\main\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 686, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"c:\\Users\\USER\\anaconda3\\envs\\main\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 1162, in fit\n",
      "    solver = _check_solver(self.solver, self.penalty, self.dual)\n",
      "  File \"c:\\Users\\USER\\anaconda3\\envs\\main\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 54, in _check_solver\n",
      "    raise ValueError(\n",
      "ValueError: Solver lbfgs supports only 'l2' or 'none' penalties, got elasticnet penalty.\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "200 fits failed with the following error:\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\USER\\anaconda3\\envs\\main\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 686, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"c:\\Users\\USER\\anaconda3\\envs\\main\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 1162, in fit\n",
      "    solver = _check_solver(self.solver, self.penalty, self.dual)\n",
      "  File \"c:\\Users\\USER\\anaconda3\\envs\\main\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 64, in _check_solver\n",
      "    raise ValueError(\n",
      "ValueError: Only 'saga' solver supports elasticnet penalty, got solver=liblinear.\n",
      "\n",
      "One or more of the test scores are non-finite: [       nan        nan 0.48888889 0.51111111 0.57777778 0.57777778\n",
      " 0.73333333 0.55555556        nan        nan        nan 0.48888889\n",
      "        nan        nan 0.48888889 0.51111111 0.57777778 0.57777778\n",
      " 0.73333333 0.6               nan        nan        nan 0.51111111\n",
      "        nan        nan 0.48888889 0.48888889 0.57777778 0.57777778\n",
      " 0.73333333 0.57777778        nan        nan        nan 0.44444444\n",
      "        nan        nan 0.48888889 0.46666667 0.57777778 0.57777778\n",
      " 0.73333333 0.48888889        nan        nan        nan 0.48888889\n",
      "        nan        nan 0.48888889 0.48888889 0.57777778 0.57777778\n",
      " 0.73333333 0.57777778        nan        nan        nan 0.51111111\n",
      "        nan        nan 0.48888889 0.46666667 0.57777778 0.57777778\n",
      " 0.73333333 0.57777778        nan        nan        nan 0.48888889\n",
      "        nan        nan 0.48888889 0.53333333 0.57777778 0.57777778\n",
      " 0.73333333 0.57777778        nan        nan        nan 0.46666667\n",
      "        nan        nan 0.48888889 0.46666667 0.57777778 0.57777778\n",
      " 0.73333333 0.57777778        nan        nan        nan 0.48888889\n",
      "        nan        nan 0.48888889 0.53333333 0.57777778 0.57777778\n",
      " 0.73333333 0.57777778        nan        nan        nan 0.51111111\n",
      "        nan        nan 0.48888889 0.51111111 0.57777778 0.57777778\n",
      " 0.73333333 0.57777778        nan        nan        nan 0.48888889\n",
      "        nan        nan 0.48888889 0.51111111 0.71111111 0.71111111\n",
      " 0.73333333 0.71111111        nan        nan        nan 0.46666667\n",
      "        nan        nan 0.48888889 0.51111111 0.71111111 0.71111111\n",
      " 0.73333333 0.71111111        nan        nan        nan 0.48888889\n",
      "        nan        nan 0.48888889 0.51111111 0.71111111 0.71111111\n",
      " 0.73333333 0.71111111        nan        nan        nan 0.53333333\n",
      "        nan        nan 0.48888889 0.51111111 0.71111111 0.71111111\n",
      " 0.73333333 0.71111111        nan        nan        nan 0.51111111\n",
      "        nan        nan 0.48888889 0.53333333 0.71111111 0.71111111\n",
      " 0.73333333 0.71111111        nan        nan        nan 0.53333333\n",
      "        nan        nan 0.64444444 0.57777778 0.75555556 0.75555556\n",
      " 0.73333333 0.75555556        nan        nan        nan 0.75555556\n",
      "        nan        nan 0.64444444 0.57777778 0.75555556 0.75555556\n",
      " 0.73333333 0.75555556        nan        nan        nan 0.62222222\n",
      "        nan        nan 0.64444444 0.55555556 0.75555556 0.75555556\n",
      " 0.73333333 0.75555556        nan        nan        nan 0.66666667\n",
      "        nan        nan 0.64444444 0.55555556 0.75555556 0.75555556\n",
      " 0.73333333 0.75555556        nan        nan        nan 0.73333333\n",
      "        nan        nan 0.64444444 0.55555556 0.75555556 0.75555556\n",
      " 0.73333333 0.75555556        nan        nan        nan 0.75555556\n",
      "        nan        nan 0.77777778 0.75555556 0.75555556 0.75555556\n",
      " 0.75555556 0.75555556        nan        nan        nan 0.75555556\n",
      "        nan        nan 0.77777778 0.75555556 0.75555556 0.75555556\n",
      " 0.75555556 0.75555556        nan        nan        nan 0.75555556\n",
      "        nan        nan 0.77777778 0.75555556 0.75555556 0.75555556\n",
      " 0.75555556 0.75555556        nan        nan        nan 0.75555556\n",
      "        nan        nan 0.77777778 0.75555556 0.75555556 0.75555556\n",
      " 0.75555556 0.75555556        nan        nan        nan 0.75555556\n",
      "        nan        nan 0.77777778 0.75555556 0.75555556 0.75555556\n",
      " 0.75555556 0.75555556        nan        nan        nan 0.75555556\n",
      "        nan        nan 0.71111111 0.71111111 0.73333333 0.73333333\n",
      " 0.73333333 0.73333333        nan        nan        nan 0.73333333\n",
      "        nan        nan 0.71111111 0.71111111 0.73333333 0.73333333\n",
      " 0.73333333 0.73333333        nan        nan        nan 0.71111111\n",
      "        nan        nan 0.71111111 0.71111111 0.73333333 0.73333333\n",
      " 0.73333333 0.73333333        nan        nan        nan 0.71111111\n",
      "        nan        nan 0.71111111 0.71111111 0.73333333 0.73333333\n",
      " 0.73333333 0.73333333        nan        nan        nan 0.71111111\n",
      "        nan        nan 0.71111111 0.71111111 0.73333333 0.73333333\n",
      " 0.73333333 0.73333333        nan        nan        nan 0.73333333\n",
      "        nan        nan 0.73333333 0.73333333 0.73333333 0.73333333\n",
      " 0.73333333 0.73333333        nan        nan        nan 0.73333333\n",
      "        nan        nan 0.73333333 0.73333333 0.73333333 0.73333333\n",
      " 0.73333333 0.73333333        nan        nan        nan 0.73333333\n",
      "        nan        nan 0.73333333 0.73333333 0.73333333 0.73333333\n",
      " 0.73333333 0.73333333        nan        nan        nan 0.73333333\n",
      "        nan        nan 0.73333333 0.73333333 0.73333333 0.73333333\n",
      " 0.73333333 0.73333333        nan        nan        nan 0.73333333\n",
      "        nan        nan 0.73333333 0.73333333 0.73333333 0.73333333\n",
      " 0.73333333 0.73333333        nan        nan        nan 0.73333333\n",
      "        nan        nan 0.73333333 0.73333333 0.73333333 0.73333333\n",
      " 0.73333333 0.73333333        nan        nan        nan 0.73333333\n",
      "        nan        nan 0.73333333 0.73333333 0.73333333 0.73333333\n",
      " 0.73333333 0.73333333        nan        nan        nan 0.73333333\n",
      "        nan        nan 0.73333333 0.73333333 0.73333333 0.73333333\n",
      " 0.73333333 0.73333333        nan        nan        nan 0.73333333\n",
      "        nan        nan 0.73333333 0.73333333 0.73333333 0.73333333\n",
      " 0.73333333 0.73333333        nan        nan        nan 0.73333333\n",
      "        nan        nan 0.73333333 0.73333333 0.73333333 0.73333333\n",
      " 0.73333333 0.73333333        nan        nan        nan 0.73333333]\n",
      "l1_ratio parameter is only used when penalty is 'elasticnet'. Got (penalty=l1)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'Dummy': {'test_acc': 0.5,\n",
      "           'test_acc_bal': 0.5,\n",
      "           'test_cohen_kappa': 0.0,\n",
      "           'test_f1': 0.6666666666666666,\n",
      "           'uncertainty_mean': 0.5111111111111111,\n",
      "           'uncertainty_std': 0.0,\n",
      "           'val': 0.4444444444444445},\n",
      " 'LogReg': {'test_acc': 0.8125,\n",
      "            'test_acc_bal': 0.8125,\n",
      "            'test_cohen_kappa': 0.625,\n",
      "            'test_f1': 0.7692307692307693,\n",
      "            'uncertainty_mean': 0.7321530819292434,\n",
      "            'uncertainty_std': 0.0963999552182735,\n",
      "            'val': 0.7777777777777777}}\n",
      "{'Dummy': {'test_acc': 0.5,\n",
      "           'test_acc_bal': 0.5,\n",
      "           'test_cohen_kappa': 0.0,\n",
      "           'test_f1': 0.6666666666666666,\n",
      "           'uncertainty_mean': 0.5111111111111111,\n",
      "           'uncertainty_std': 0.0,\n",
      "           'val': 0.4444444444444445},\n",
      " 'LogReg': {'test_acc': 0.8125,\n",
      "            'test_acc_bal': 0.8125,\n",
      "            'test_cohen_kappa': 0.625,\n",
      "            'test_f1': 0.7692307692307693,\n",
      "            'uncertainty_mean': 0.7321530819292434,\n",
      "            'uncertainty_std': 0.0963999552182735,\n",
      "            'val': 0.7777777777777777},\n",
      " 'SVM': {'test_acc': 0.8125,\n",
      "         'test_acc_bal': 0.8125,\n",
      "         'test_cohen_kappa': 0.625,\n",
      "         'test_f1': 0.7999999999999999,\n",
      "         'uncertainty_mean': 0.7516771610573477,\n",
      "         'uncertainty_std': 0.09882808355352021,\n",
      "         'val': 0.7555555555555555}}\n",
      "{'Dummy': {'test_acc': 0.5,\n",
      "           'test_acc_bal': 0.5,\n",
      "           'test_cohen_kappa': 0.0,\n",
      "           'test_f1': 0.6666666666666666,\n",
      "           'uncertainty_mean': 0.5111111111111111,\n",
      "           'uncertainty_std': 0.0,\n",
      "           'val': 0.4444444444444445},\n",
      " 'LogReg': {'test_acc': 0.8125,\n",
      "            'test_acc_bal': 0.8125,\n",
      "            'test_cohen_kappa': 0.625,\n",
      "            'test_f1': 0.7692307692307693,\n",
      "            'uncertainty_mean': 0.7321530819292434,\n",
      "            'uncertainty_std': 0.0963999552182735,\n",
      "            'val': 0.7777777777777777},\n",
      " 'RandomForest': {'test_acc': 0.8125,\n",
      "                  'test_acc_bal': 0.8125,\n",
      "                  'test_cohen_kappa': 0.625,\n",
      "                  'test_f1': 0.7999999999999999,\n",
      "                  'uncertainty_mean': 0.9383333333333334,\n",
      "                  'uncertainty_std': 0.08094922963053927,\n",
      "                  'val': 0.7999999999999999},\n",
      " 'SVM': {'test_acc': 0.8125,\n",
      "         'test_acc_bal': 0.8125,\n",
      "         'test_cohen_kappa': 0.625,\n",
      "         'test_f1': 0.7999999999999999,\n",
      "         'uncertainty_mean': 0.7516771610573477,\n",
      "         'uncertainty_std': 0.09882808355352021,\n",
      "         'val': 0.7555555555555555}}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Stochastic Optimizer: Maximum iterations (1000) reached and the optimization hasn't converged yet.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'Dummy': {'test_acc': 0.5,\n",
      "           'test_acc_bal': 0.5,\n",
      "           'test_cohen_kappa': 0.0,\n",
      "           'test_f1': 0.6666666666666666,\n",
      "           'uncertainty_mean': 0.5111111111111111,\n",
      "           'uncertainty_std': 0.0,\n",
      "           'val': 0.4444444444444445},\n",
      " 'LogReg': {'test_acc': 0.8125,\n",
      "            'test_acc_bal': 0.8125,\n",
      "            'test_cohen_kappa': 0.625,\n",
      "            'test_f1': 0.7692307692307693,\n",
      "            'uncertainty_mean': 0.7321530819292434,\n",
      "            'uncertainty_std': 0.0963999552182735,\n",
      "            'val': 0.7777777777777777},\n",
      " 'MLP': {'test_acc': 0.75,\n",
      "         'test_acc_bal': 0.75,\n",
      "         'test_cohen_kappa': 0.5,\n",
      "         'test_f1': 0.7142857142857143,\n",
      "         'uncertainty_mean': 0.8921329194144887,\n",
      "         'uncertainty_std': 0.11492997141924159,\n",
      "         'val': 0.8222222222222222},\n",
      " 'RandomForest': {'test_acc': 0.8125,\n",
      "                  'test_acc_bal': 0.8125,\n",
      "                  'test_cohen_kappa': 0.625,\n",
      "                  'test_f1': 0.7999999999999999,\n",
      "                  'uncertainty_mean': 0.9383333333333334,\n",
      "                  'uncertainty_std': 0.08094922963053927,\n",
      "                  'val': 0.7999999999999999},\n",
      " 'SVM': {'test_acc': 0.8125,\n",
      "         'test_acc_bal': 0.8125,\n",
      "         'test_cohen_kappa': 0.625,\n",
      "         'test_f1': 0.7999999999999999,\n",
      "         'uncertainty_mean': 0.7516771610573477,\n",
      "         'uncertainty_std': 0.09882808355352021,\n",
      "         'val': 0.7555555555555555}}\n"
     ]
    }
   ],
   "source": [
    "main(config_path,data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'cm' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[12], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m cm\n",
      "\u001b[1;31mNameError\u001b[0m: name 'cm' is not defined"
     ]
    }
   ],
   "source": [
    "cm"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "main",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.15"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "eeee73005f2d7c442ce01edc53b42ba7e639fc43199e10e3461fac3e3675672b"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
