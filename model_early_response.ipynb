{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pickle\n",
    "import joblib\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import pingouin as pg\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import shap\n",
    "\n",
    "from pprint import pprint\n",
    "from shutil import copyfile\n",
    "from sklearn import metrics, dummy, linear_model, ensemble, svm, neural_network\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.preprocessing import StandardScaler, LabelEncoder, OneHotEncoder\n",
    "from sklearn.model_selection import GridSearchCV, train_test_split, LeaveOneOut\n",
    "\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import roc_curve\n",
    "from sklearn.feature_selection import SelectKBest, f_classif\n",
    "\n",
    "from sklearn.datasets import load_iris"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "config_path = os.path.abspath(r'C:\\Users\\USER\\Guro_Psy_KJH Dropbox\\1.Projects\\1_anxiety_VR\\3_Data\\5_prediction_model\\1_2023_report')\n",
    "data = 'vrabes_preprocessed.pkl'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(data,'rb') as f:\n",
    "    pre = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "\n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "\n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "\n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n"
     ]
    }
   ],
   "source": [
    "# define label\n",
    "ses_01 = pd.concat([pre['ses-01_demo'], pre['ses-01_crf'], ], axis=1)\n",
    "ses_02 = pd.concat([pre['ses-02_demo'], pre['ses-02_crf']], axis=1)\n",
    "\n",
    "ses_01_anx = ses_01[ses_01['ses-01_group']==1] #31\n",
    "ses_02_anx = ses_02[ses_02['ses-02_group']==1] #26\n",
    "\n",
    "ses_01_anx['id'] = ses_01_anx.index\n",
    "ses_02_anx['id'] = ses_02_anx.index\n",
    "\n",
    "ses_01_anx['sub'] = ses_01_anx['id'].apply(lambda x: x.split('_')[0])\n",
    "ses_02_anx['sub'] = ses_02_anx['id'].apply(lambda x: x.split('_')[0])\n",
    "\n",
    "early_response = pd.merge(ses_02_anx, ses_01_anx, on = 'sub', how = 'left')\n",
    "early_response['delta'] = (early_response['ses-01_PDSS_SUM'] - early_response['ses-02_PDSS_SUM_y'])/early_response['ses-01_PDSS_SUM']\n",
    "early_response['label'] = early_response['delta']>0.25 # 12\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "\n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n"
     ]
    }
   ],
   "source": [
    "ses_01 = pd.concat([pre['ses-01_as'],pre['ses-01_demo'],pre['ses-01_hrv']], axis=1)\n",
    "ses_01_anx = ses_01[ses_01['ses-01_group']==1] \n",
    "ses_01_anx['id'] = ses_01_anx.index\n",
    "ses_01_anx['sub'] = ses_01_anx['id'].apply(lambda x: x.split('_')[0])\n",
    "target = pd.merge(early_response[['sub','label']], ses_01_anx, on = 'sub', how='left')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def main(config_path,target):\n",
    "\n",
    "    \n",
    "    # prepare result path\n",
    "    if not os.path.isdir(os.path.join(config_path,'result_dir_2')):\n",
    "        os.mkdir(os.path.join(config_path,'result_dir_2'))\n",
    "    os.mkdir(os.path.join(config_path,'result_dir_2', 'fig'))\n",
    "    os.mkdir(os.path.join(config_path,'result_dir_2', 'fig', 'confusion_matrix'))\n",
    "    os.mkdir(os.path.join(config_path,'result_dir_2', 'fig', 'roc'))\n",
    "    os.mkdir(os.path.join(config_path,'result_dir_2', 'model'))\n",
    "    os.mkdir(os.path.join(config_path,'result_dir_2', 'result'))\n",
    "    os.mkdir(os.path.join(config_path,'result_dir_2', 'result', 'cv'))\n",
    "\n",
    "    ### start analysis\n",
    "\n",
    "    # define models\n",
    "    models = [\n",
    "        ('Dummy', dummy.DummyClassifier()),\n",
    "        ('LogReg', linear_model.LogisticRegression()),\n",
    "        ('SVM', svm.SVC()),\n",
    "        ('RandomForest', ensemble.RandomForestClassifier()),\n",
    "        ('MLP', neural_network.MLPClassifier())\n",
    "    ]\n",
    "\n",
    "    # define hyperparameter search space\n",
    "    param_grid = {\n",
    "        'Dummy': {\n",
    "        \n",
    "        },\n",
    "        'LogReg': {\n",
    "            'solver': ['newton-cg', 'lbfgs', 'liblinear', 'saga'],\n",
    "            'penalty': ['l1', 'l2', 'elasticnet'],\n",
    "            'C': [float(10**x) for x in range(-4, 4)],\n",
    "            'l1_ratio': np.random.uniform(size=5),\n",
    "            'max_iter': [10000]\n",
    "        },\n",
    "        'SVM': {\n",
    "            'kernel': ['linear', 'rbf', 'poly'],\n",
    "            'C': [float(10**x) for x in range(-3, 3)],\n",
    "            'gamma': [float(10**x) for x in range(-3, 4)],\n",
    "            'probability': [True],\n",
    "        },\n",
    "        'RandomForest': {\n",
    "            'n_estimators': [10, 25, 100],            \n",
    "            'max_depth': [int(x) for x in np.linspace(10, 110, num = 11)]+[None],\n",
    "            'min_samples_split': [2, 5, 10],\n",
    "            'min_samples_leaf': [1, 2, 4],\n",
    "            'bootstrap': [True, False]\n",
    "        },\n",
    "        'MLP': {\n",
    "            'hidden_layer_sizes': [10, 50, 100],\n",
    "            'solver': ['lbfgs', 'sgd', 'adam'],\n",
    "            'alpha': [float(10**x) for x in range(-2,2)],\n",
    "            'batch_size': [int(2**x) for x in range(3, 5)],\n",
    "            'learning_rate': ['constant', 'invscaling', 'adaptive'],\n",
    "            'max_iter': [1000],\n",
    "            'warm_start': [False, True]\n",
    "        },\n",
    "        \n",
    "    }\n",
    "\n",
    "    # prepare input-label data pairs    \n",
    "    X_pre = target.drop(['ses-01_group','label','sub','id'],axis =1).to_numpy()\n",
    "    y = target['label'].to_numpy()\n",
    "\n",
    "    # # feature selection\n",
    "    feat_sel = SelectKBest(f_classif, k=5)\n",
    "    X = feat_sel.fit_transform(X_pre,y)\n",
    "    selected_feature = {0:feat_sel.get_feature_names_out()}    \n",
    "    pd.DataFrame.from_dict(selected_feature).to_csv(os.path.join(config_path,'result_dir_2', 'result', 'selected_features.csv'))\n",
    "    \n",
    "\n",
    "    # standardize input data\n",
    "    scaler = StandardScaler()\n",
    "    scaler.fit(X)\n",
    "    X = scaler.transform(X)\n",
    "    \n",
    "    # LOOCV \n",
    "    loo = LeaveOneOut()\n",
    "\n",
    "    # start model training\n",
    "    result_dict = {}\n",
    "    weight_importance_dict = {}    \n",
    "\n",
    "    for model_name, model in models:        \n",
    "        model_hyperparamsearch = GridSearchCV(estimator=model, param_grid=param_grid[model_name], cv=loo, verbose=0, n_jobs=-1)\n",
    "        model_hyperparamsearch.fit(X, y)\n",
    "        best_estimator = model_hyperparamsearch.best_estimator_\n",
    "\n",
    "        # get prediction on the test dataset\n",
    "        pred_test = model_hyperparamsearch.predict(X)\n",
    "\n",
    "        # keept test performance results\n",
    "        result_dict[model_name] = {}\n",
    "        result_dict[model_name]['uncertainty_mean'] = best_estimator.predict_proba(X).max(1).mean()\n",
    "        result_dict[model_name]['uncertainty_std'] = best_estimator.predict_proba(X).max(1).std()\n",
    "        result_dict[model_name]['test_acc'] = metrics.accuracy_score(y, pred_test)\n",
    "        result_dict[model_name]['test_acc_bal'] = metrics.balanced_accuracy_score(y, pred_test)\n",
    "        result_dict[model_name]['test_f1'] = metrics.f1_score(y, pred_test, average='binary')\n",
    "        result_dict[model_name]['test_cohen_kappa'] = metrics.cohen_kappa_score(y, pred_test, labels=y)\n",
    "        result_dict[model_name]['val'] = model_hyperparamsearch.best_score_\n",
    "        pprint(result_dict)\n",
    "\n",
    "        # plot confusion matrix\n",
    "        cm = metrics.confusion_matrix(y, pred_test, labels=y)\n",
    "        cm = cm/float(cm.sum()) # convert to ratio\n",
    "        ax = sns.heatmap(cm, annot=True, xticklabels=y, yticklabels=y, cmap='Blues', fmt='.2f', square=True)\n",
    "        plt.suptitle(f'{model_name}')\n",
    "        plt.tight_layout()\n",
    "        plt.savefig(os.path.join(config_path,'result_dir_2','fig', 'confusion_matrix', model_name+'.png'))\n",
    "        plt.close()\n",
    "\n",
    "        # plot ROC curve\n",
    "        fpr, tpr, thresholds = roc_curve(y,pred_test)\n",
    "        plt.plot(fpr, tpr, 'o-', label=\"Decision Tree\")\n",
    "        plt.plot([0, 1], [0, 1], 'k--', label=\"random guess\")\n",
    "        plt.xlabel('Fall-Out')\n",
    "        plt.ylabel('Recall')\n",
    "        plt.title('Receiver operating characteristic example')\n",
    "        plt.savefig(os.path.join(config_path,'result_dir_2', 'fig', 'roc', model_name+'.png'))\n",
    "        plt.close()\n",
    "\n",
    "        # save model and results\n",
    "        joblib.dump(best_estimator, os.path.join(config_path,'result_dir_2', 'model', model_name+'.joblib'))\n",
    "        pd.DataFrame.from_dict(model_hyperparamsearch.cv_results_).to_csv(os.path.join(config_path,'result_dir_2', 'result', 'cv', model_name+'.csv'))\n",
    "        pd.DataFrame.from_dict(result_dict).T.to_csv(os.path.join(config_path,'result_dir_2', 'result', 'result.csv'))\n",
    "        pd.DataFrame.from_dict(weight_importance_dict).T.to_csv(os.path.join(config_path,'result_dir_2', 'result', 'importance_weight.csv'))  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Features [ 25  26  27  43  61  62  63  79  97  98  99 115 133 134 135 151 169 170\n",
      " 171 187 205 206 207 223 241 242 243 259 277 278 279 295 313 314 315 331\n",
      " 349 350 351 367] are constant.\n",
      "invalid value encountered in divide\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'Dummy': {'test_acc': 0.5384615384615384,\n",
      "           'test_acc_bal': 0.5,\n",
      "           'test_cohen_kappa': 0.0,\n",
      "           'test_f1': 0.0,\n",
      "           'uncertainty_mean': 0.5384615384615384,\n",
      "           'uncertainty_std': 0.0,\n",
      "           'val': 0.5384615384615384}}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "5200 fits failed out of a total of 12480.\n",
      "The score on these train-test partitions for these parameters will be set to nan.\n",
      "If these failures are not expected, you can try to debug them by setting error_score='raise'.\n",
      "\n",
      "Below are more details about the failures:\n",
      "--------------------------------------------------------------------------------\n",
      "1040 fits failed with the following error:\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\USER\\anaconda3\\envs\\main\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 686, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"c:\\Users\\USER\\anaconda3\\envs\\main\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 1162, in fit\n",
      "    solver = _check_solver(self.solver, self.penalty, self.dual)\n",
      "  File \"c:\\Users\\USER\\anaconda3\\envs\\main\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 54, in _check_solver\n",
      "    raise ValueError(\n",
      "ValueError: Solver newton-cg supports only 'l2' or 'none' penalties, got l1 penalty.\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "1040 fits failed with the following error:\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\USER\\anaconda3\\envs\\main\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 686, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"c:\\Users\\USER\\anaconda3\\envs\\main\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 1162, in fit\n",
      "    solver = _check_solver(self.solver, self.penalty, self.dual)\n",
      "  File \"c:\\Users\\USER\\anaconda3\\envs\\main\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 54, in _check_solver\n",
      "    raise ValueError(\n",
      "ValueError: Solver lbfgs supports only 'l2' or 'none' penalties, got l1 penalty.\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "1040 fits failed with the following error:\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\USER\\anaconda3\\envs\\main\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 686, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"c:\\Users\\USER\\anaconda3\\envs\\main\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 1162, in fit\n",
      "    solver = _check_solver(self.solver, self.penalty, self.dual)\n",
      "  File \"c:\\Users\\USER\\anaconda3\\envs\\main\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 54, in _check_solver\n",
      "    raise ValueError(\n",
      "ValueError: Solver newton-cg supports only 'l2' or 'none' penalties, got elasticnet penalty.\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "1040 fits failed with the following error:\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\USER\\anaconda3\\envs\\main\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 686, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"c:\\Users\\USER\\anaconda3\\envs\\main\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 1162, in fit\n",
      "    solver = _check_solver(self.solver, self.penalty, self.dual)\n",
      "  File \"c:\\Users\\USER\\anaconda3\\envs\\main\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 54, in _check_solver\n",
      "    raise ValueError(\n",
      "ValueError: Solver lbfgs supports only 'l2' or 'none' penalties, got elasticnet penalty.\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "1040 fits failed with the following error:\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\USER\\anaconda3\\envs\\main\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 686, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"c:\\Users\\USER\\anaconda3\\envs\\main\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 1162, in fit\n",
      "    solver = _check_solver(self.solver, self.penalty, self.dual)\n",
      "  File \"c:\\Users\\USER\\anaconda3\\envs\\main\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 64, in _check_solver\n",
      "    raise ValueError(\n",
      "ValueError: Only 'saga' solver supports elasticnet penalty, got solver=liblinear.\n",
      "\n",
      "One or more of the test scores are non-finite: [       nan        nan 0.53846154 0.42307692 0.53846154 0.53846154\n",
      " 0.92307692 0.53846154        nan        nan        nan 0.34615385\n",
      "        nan        nan 0.53846154 0.46153846 0.53846154 0.53846154\n",
      " 0.92307692 0.53846154        nan        nan        nan 0.53846154\n",
      "        nan        nan 0.53846154 0.46153846 0.53846154 0.53846154\n",
      " 0.92307692 0.53846154        nan        nan        nan 0.57692308\n",
      "        nan        nan 0.53846154 0.42307692 0.53846154 0.53846154\n",
      " 0.92307692 0.46153846        nan        nan        nan 0.5\n",
      "        nan        nan 0.53846154 0.30769231 0.53846154 0.53846154\n",
      " 0.92307692 0.53846154        nan        nan        nan 0.57692308\n",
      "        nan        nan 0.53846154 0.38461538 0.53846154 0.53846154\n",
      " 0.92307692 0.53846154        nan        nan        nan 0.38461538\n",
      "        nan        nan 0.53846154 0.53846154 0.53846154 0.53846154\n",
      " 0.92307692 0.53846154        nan        nan        nan 0.42307692\n",
      "        nan        nan 0.53846154 0.38461538 0.53846154 0.53846154\n",
      " 0.92307692 0.53846154        nan        nan        nan 0.34615385\n",
      "        nan        nan 0.53846154 0.53846154 0.53846154 0.53846154\n",
      " 0.92307692 0.53846154        nan        nan        nan 0.5\n",
      "        nan        nan 0.53846154 0.42307692 0.53846154 0.53846154\n",
      " 0.92307692 0.53846154        nan        nan        nan 0.42307692\n",
      "        nan        nan 0.53846154 0.5        0.53846154 0.53846154\n",
      " 0.92307692 0.53846154        nan        nan        nan 0.34615385\n",
      "        nan        nan 0.53846154 0.23076923 0.53846154 0.53846154\n",
      " 0.92307692 0.53846154        nan        nan        nan 0.42307692\n",
      "        nan        nan 0.53846154 0.5        0.53846154 0.53846154\n",
      " 0.92307692 0.53846154        nan        nan        nan 0.30769231\n",
      "        nan        nan 0.53846154 0.38461538 0.53846154 0.53846154\n",
      " 0.92307692 0.53846154        nan        nan        nan 0.46153846\n",
      "        nan        nan 0.53846154 0.34615385 0.53846154 0.53846154\n",
      " 0.92307692 0.53846154        nan        nan        nan 0.57692308\n",
      "        nan        nan 0.53846154 0.46153846 0.92307692 0.92307692\n",
      " 0.88461538 0.92307692        nan        nan        nan 0.84615385\n",
      "        nan        nan 0.53846154 0.5        0.92307692 0.92307692\n",
      " 0.88461538 0.92307692        nan        nan        nan 0.53846154\n",
      "        nan        nan 0.53846154 0.53846154 0.92307692 0.92307692\n",
      " 0.88461538 0.92307692        nan        nan        nan 0.84615385\n",
      "        nan        nan 0.53846154 0.5        0.92307692 0.92307692\n",
      " 0.88461538 0.92307692        nan        nan        nan 0.61538462\n",
      "        nan        nan 0.53846154 0.5        0.92307692 0.92307692\n",
      " 0.88461538 0.92307692        nan        nan        nan 0.84615385\n",
      "        nan        nan 0.80769231 0.84615385 0.88461538 0.88461538\n",
      " 0.84615385 0.88461538        nan        nan        nan 0.88461538\n",
      "        nan        nan 0.80769231 0.84615385 0.88461538 0.88461538\n",
      " 0.84615385 0.88461538        nan        nan        nan 0.84615385\n",
      "        nan        nan 0.80769231 0.84615385 0.88461538 0.88461538\n",
      " 0.84615385 0.88461538        nan        nan        nan 0.88461538\n",
      "        nan        nan 0.80769231 0.84615385 0.88461538 0.88461538\n",
      " 0.84615385 0.88461538        nan        nan        nan 0.84615385\n",
      "        nan        nan 0.80769231 0.84615385 0.88461538 0.88461538\n",
      " 0.84615385 0.88461538        nan        nan        nan 0.88461538\n",
      "        nan        nan 0.84615385 0.84615385 0.84615385 0.84615385\n",
      " 0.84615385 0.84615385        nan        nan        nan 0.84615385\n",
      "        nan        nan 0.84615385 0.84615385 0.84615385 0.84615385\n",
      " 0.84615385 0.84615385        nan        nan        nan 0.84615385\n",
      "        nan        nan 0.84615385 0.84615385 0.84615385 0.84615385\n",
      " 0.84615385 0.84615385        nan        nan        nan 0.84615385\n",
      "        nan        nan 0.84615385 0.84615385 0.84615385 0.84615385\n",
      " 0.84615385 0.84615385        nan        nan        nan 0.84615385\n",
      "        nan        nan 0.84615385 0.84615385 0.84615385 0.84615385\n",
      " 0.84615385 0.84615385        nan        nan        nan 0.84615385\n",
      "        nan        nan 0.80769231 0.80769231 0.84615385 0.84615385\n",
      " 0.84615385 0.84615385        nan        nan        nan 0.84615385\n",
      "        nan        nan 0.80769231 0.80769231 0.84615385 0.84615385\n",
      " 0.84615385 0.84615385        nan        nan        nan 0.84615385\n",
      "        nan        nan 0.80769231 0.80769231 0.84615385 0.84615385\n",
      " 0.84615385 0.84615385        nan        nan        nan 0.84615385\n",
      "        nan        nan 0.80769231 0.80769231 0.84615385 0.84615385\n",
      " 0.84615385 0.84615385        nan        nan        nan 0.84615385\n",
      "        nan        nan 0.80769231 0.80769231 0.84615385 0.84615385\n",
      " 0.84615385 0.84615385        nan        nan        nan 0.84615385\n",
      "        nan        nan 0.80769231 0.80769231 0.80769231 0.80769231\n",
      " 0.80769231 0.80769231        nan        nan        nan 0.80769231\n",
      "        nan        nan 0.80769231 0.80769231 0.80769231 0.80769231\n",
      " 0.80769231 0.80769231        nan        nan        nan 0.80769231\n",
      "        nan        nan 0.80769231 0.80769231 0.80769231 0.80769231\n",
      " 0.80769231 0.80769231        nan        nan        nan 0.80769231\n",
      "        nan        nan 0.80769231 0.80769231 0.80769231 0.80769231\n",
      " 0.80769231 0.80769231        nan        nan        nan 0.80769231\n",
      "        nan        nan 0.80769231 0.80769231 0.80769231 0.80769231\n",
      " 0.80769231 0.80769231        nan        nan        nan 0.80769231]\n",
      "l1_ratio parameter is only used when penalty is 'elasticnet'. Got (penalty=l2)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'Dummy': {'test_acc': 0.5384615384615384,\n",
      "           'test_acc_bal': 0.5,\n",
      "           'test_cohen_kappa': 0.0,\n",
      "           'test_f1': 0.0,\n",
      "           'uncertainty_mean': 0.5384615384615384,\n",
      "           'uncertainty_std': 0.0,\n",
      "           'val': 0.5384615384615384},\n",
      " 'LogReg': {'test_acc': 0.9230769230769231,\n",
      "            'test_acc_bal': 0.9226190476190477,\n",
      "            'test_cohen_kappa': 0.8452380952380952,\n",
      "            'test_f1': 0.9166666666666666,\n",
      "            'uncertainty_mean': 0.500482457084289,\n",
      "            'uncertainty_std': 0.00032317799321856616,\n",
      "            'val': 0.9230769230769231}}\n",
      "{'Dummy': {'test_acc': 0.5384615384615384,\n",
      "           'test_acc_bal': 0.5,\n",
      "           'test_cohen_kappa': 0.0,\n",
      "           'test_f1': 0.0,\n",
      "           'uncertainty_mean': 0.5384615384615384,\n",
      "           'uncertainty_std': 0.0,\n",
      "           'val': 0.5384615384615384},\n",
      " 'LogReg': {'test_acc': 0.9230769230769231,\n",
      "            'test_acc_bal': 0.9226190476190477,\n",
      "            'test_cohen_kappa': 0.8452380952380952,\n",
      "            'test_f1': 0.9166666666666666,\n",
      "            'uncertainty_mean': 0.500482457084289,\n",
      "            'uncertainty_std': 0.00032317799321856616,\n",
      "            'val': 0.9230769230769231},\n",
      " 'SVM': {'test_acc': 0.9615384615384616,\n",
      "         'test_acc_bal': 0.9583333333333333,\n",
      "         'test_cohen_kappa': 0.9221556886227544,\n",
      "         'test_f1': 0.9565217391304348,\n",
      "         'uncertainty_mean': 0.8270387068316043,\n",
      "         'uncertainty_std': 0.13793276857826728,\n",
      "         'val': 0.9230769230769231}}\n",
      "{'Dummy': {'test_acc': 0.5384615384615384,\n",
      "           'test_acc_bal': 0.5,\n",
      "           'test_cohen_kappa': 0.0,\n",
      "           'test_f1': 0.0,\n",
      "           'uncertainty_mean': 0.5384615384615384,\n",
      "           'uncertainty_std': 0.0,\n",
      "           'val': 0.5384615384615384},\n",
      " 'LogReg': {'test_acc': 0.9230769230769231,\n",
      "            'test_acc_bal': 0.9226190476190477,\n",
      "            'test_cohen_kappa': 0.8452380952380952,\n",
      "            'test_f1': 0.9166666666666666,\n",
      "            'uncertainty_mean': 0.500482457084289,\n",
      "            'uncertainty_std': 0.00032317799321856616,\n",
      "            'val': 0.9230769230769231},\n",
      " 'RandomForest': {'test_acc': 0.9615384615384616,\n",
      "                  'test_acc_bal': 0.9583333333333333,\n",
      "                  'test_cohen_kappa': 0.9221556886227544,\n",
      "                  'test_f1': 0.9565217391304348,\n",
      "                  'uncertainty_mean': 0.8961538461538461,\n",
      "                  'uncertainty_std': 0.1315024692098366,\n",
      "                  'val': 0.8846153846153846},\n",
      " 'SVM': {'test_acc': 0.9615384615384616,\n",
      "         'test_acc_bal': 0.9583333333333333,\n",
      "         'test_cohen_kappa': 0.9221556886227544,\n",
      "         'test_f1': 0.9565217391304348,\n",
      "         'uncertainty_mean': 0.8270387068316043,\n",
      "         'uncertainty_std': 0.13793276857826728,\n",
      "         'val': 0.9230769230769231}}\n",
      "{'Dummy': {'test_acc': 0.5384615384615384,\n",
      "           'test_acc_bal': 0.5,\n",
      "           'test_cohen_kappa': 0.0,\n",
      "           'test_f1': 0.0,\n",
      "           'uncertainty_mean': 0.5384615384615384,\n",
      "           'uncertainty_std': 0.0,\n",
      "           'val': 0.5384615384615384},\n",
      " 'LogReg': {'test_acc': 0.9230769230769231,\n",
      "            'test_acc_bal': 0.9226190476190477,\n",
      "            'test_cohen_kappa': 0.8452380952380952,\n",
      "            'test_f1': 0.9166666666666666,\n",
      "            'uncertainty_mean': 0.500482457084289,\n",
      "            'uncertainty_std': 0.00032317799321856616,\n",
      "            'val': 0.9230769230769231},\n",
      " 'MLP': {'test_acc': 0.9230769230769231,\n",
      "         'test_acc_bal': 0.9226190476190477,\n",
      "         'test_cohen_kappa': 0.8452380952380952,\n",
      "         'test_f1': 0.9166666666666666,\n",
      "         'uncertainty_mean': 0.9401473433383523,\n",
      "         'uncertainty_std': 0.11349508993416153,\n",
      "         'val': 0.9230769230769231},\n",
      " 'RandomForest': {'test_acc': 0.9615384615384616,\n",
      "                  'test_acc_bal': 0.9583333333333333,\n",
      "                  'test_cohen_kappa': 0.9221556886227544,\n",
      "                  'test_f1': 0.9565217391304348,\n",
      "                  'uncertainty_mean': 0.8961538461538461,\n",
      "                  'uncertainty_std': 0.1315024692098366,\n",
      "                  'val': 0.8846153846153846},\n",
      " 'SVM': {'test_acc': 0.9615384615384616,\n",
      "         'test_acc_bal': 0.9583333333333333,\n",
      "         'test_cohen_kappa': 0.9221556886227544,\n",
      "         'test_f1': 0.9565217391304348,\n",
      "         'uncertainty_mean': 0.8270387068316043,\n",
      "         'uncertainty_std': 0.13793276857826728,\n",
      "         'val': 0.9230769230769231}}\n"
     ]
    }
   ],
   "source": [
    "main(config_path,target)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "main",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.15"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "eeee73005f2d7c442ce01edc53b42ba7e639fc43199e10e3461fac3e3675672b"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
