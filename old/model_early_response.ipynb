{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pickle\n",
    "import joblib\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import pingouin as pg\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import shap\n",
    "\n",
    "from pprint import pprint\n",
    "from shutil import copyfile\n",
    "from sklearn import metrics, dummy, linear_model, ensemble, svm, neural_network\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.preprocessing import StandardScaler, LabelEncoder, OneHotEncoder\n",
    "from sklearn.model_selection import GridSearchCV, train_test_split, LeaveOneOut\n",
    "\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import roc_curve\n",
    "from sklearn.feature_selection import SelectKBest, f_classif\n",
    "\n",
    "from sklearn.datasets import load_iris"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "config_path = os.path.abspath(r'C:\\Users\\USER\\Guro_Psy_KJH Dropbox\\1.Projects\\1_anxiety_VR\\3_Data\\5_prediction_model\\1_2023_report')\n",
    "data = 'vrabes_preprocessed.pkl'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(data,'rb') as f:\n",
    "    pre = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "\n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "\n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "\n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n"
     ]
    }
   ],
   "source": [
    "# define label\n",
    "ses_01 = pd.concat([pre['ses-01_demo'], pre['ses-01_crf'], ], axis=1)\n",
    "ses_02 = pd.concat([pre['ses-02_demo'], pre['ses-02_crf']], axis=1)\n",
    "\n",
    "ses_01_anx = ses_01[ses_01['ses-01_group']==1] #31\n",
    "ses_02_anx = ses_02[ses_02['ses-02_group']==1] #26\n",
    "\n",
    "ses_01_anx['id'] = ses_01_anx.index\n",
    "ses_02_anx['id'] = ses_02_anx.index\n",
    "\n",
    "ses_01_anx['sub'] = ses_01_anx['id'].apply(lambda x: x.split('_')[0])\n",
    "ses_02_anx['sub'] = ses_02_anx['id'].apply(lambda x: x.split('_')[0])\n",
    "\n",
    "early_response = pd.merge(ses_02_anx, ses_01_anx, on = 'sub', how = 'left')\n",
    "early_response['delta'] = (early_response['ses-01_PDSS_SUM'] - early_response['ses-02_PDSS_SUM_y'])/early_response['ses-01_PDSS_SUM']\n",
    "early_response['label'] = early_response['delta']>0.25 # 12\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "\n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n"
     ]
    }
   ],
   "source": [
    "ses_01 = pd.concat([pre['ses-01_as'],pre['ses-01_demo'],pre['ses-01_hrv']], axis=1)\n",
    "ses_01_anx = ses_01[ses_01['ses-01_group']==1] \n",
    "ses_01_anx['id'] = ses_01_anx.index\n",
    "ses_01_anx['sub'] = ses_01_anx['id'].apply(lambda x: x.split('_')[0])\n",
    "target = pd.merge(early_response[['sub','label']], ses_01_anx, on = 'sub', how='left')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def main(config_path,target):\n",
    "\n",
    "    \n",
    "    # prepare result path\n",
    "    if not os.path.isdir(os.path.join(config_path,'reult_dir_3')):\n",
    "        os.mkdir(os.path.join(config_path,'reult_dir_3'))\n",
    "    os.mkdir(os.path.join(config_path,'reult_dir_3', 'fig'))\n",
    "    os.mkdir(os.path.join(config_path,'reult_dir_3', 'fig', 'confusion_matrix'))\n",
    "    os.mkdir(os.path.join(config_path,'reult_dir_3', 'fig', 'roc'))\n",
    "    os.mkdir(os.path.join(config_path,'reult_dir_3', 'model'))\n",
    "    os.mkdir(os.path.join(config_path,'reult_dir_3', 'result'))\n",
    "    os.mkdir(os.path.join(config_path,'reult_dir_3', 'result', 'cv'))\n",
    "\n",
    "    ### start analysis\n",
    "\n",
    "    # define models\n",
    "    models = [\n",
    "        ('Dummy', dummy.DummyClassifier()),\n",
    "        ('LogReg', linear_model.LogisticRegression()),\n",
    "        ('SVM', svm.SVC()),\n",
    "        ('RandomForest', ensemble.RandomForestClassifier()),\n",
    "        ('MLP', neural_network.MLPClassifier())\n",
    "    ]\n",
    "\n",
    "    # define hyperparameter search space\n",
    "    param_grid = {\n",
    "        'Dummy': {\n",
    "        \n",
    "        },\n",
    "        'LogReg': {\n",
    "            'solver': ['newton-cg', 'lbfgs', 'liblinear', 'saga'],\n",
    "            'penalty': ['l1', 'l2', 'elasticnet'],\n",
    "            'C': [float(10**x) for x in range(-4, 4)],\n",
    "            'l1_ratio': np.random.uniform(size=5),\n",
    "            'max_iter': [10000]\n",
    "        },\n",
    "        'SVM': {\n",
    "            'kernel': ['linear', 'rbf', 'poly'],\n",
    "            'C': [float(10**x) for x in range(-3, 3)],\n",
    "            'gamma': [float(10**x) for x in range(-3, 4)],\n",
    "            'probability': [True],\n",
    "        },\n",
    "        'RandomForest': {\n",
    "            'n_estimators': [10, 25, 100],            \n",
    "            'max_depth': [int(x) for x in np.linspace(10, 110, num = 11)]+[None],\n",
    "            'min_samples_split': [2, 5, 10],\n",
    "            'min_samples_leaf': [1, 2, 4],\n",
    "            'bootstrap': [True, False]\n",
    "        },\n",
    "        'MLP': {\n",
    "            'hidden_layer_sizes': [10, 50, 100],\n",
    "            'solver': ['lbfgs', 'sgd', 'adam'],\n",
    "            'alpha': [float(10**x) for x in range(-2,2)],\n",
    "            'batch_size': [int(2**x) for x in range(3, 5)],\n",
    "            'learning_rate': ['constant', 'invscaling', 'adaptive'],\n",
    "            'max_iter': [1000],\n",
    "            'warm_start': [False, True]\n",
    "        },\n",
    "        \n",
    "    }\n",
    "\n",
    "    # prepare input-label data pairs    \n",
    "    X_pre = target.drop(['ses-01_group','label','sub','id'],axis =1).to_numpy()\n",
    "    y = target['label'].to_numpy()\n",
    "\n",
    "    # # feature selection\n",
    "    feat_sel = SelectKBest(f_classif, k=5)\n",
    "    X = feat_sel.fit_transform(X_pre,y)\n",
    "    selected_feature = {0:feat_sel.get_feature_names_out()}    \n",
    "    pd.DataFrame.from_dict(selected_feature).to_csv(os.path.join(config_path,'reult_dir_3', 'result', 'selected_features.csv'))\n",
    "    \n",
    "\n",
    "    # split data pairs into train and test datasets\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X,y,test_size=0.25, random_state=123, shuffle=True, stratify=y)\n",
    "\n",
    "    # standardize input data\n",
    "    scaler = StandardScaler()\n",
    "    scaler.fit(X_train)\n",
    "    X_train = scaler.transform(X_train)\n",
    "    X_test = scaler.transform(X_test)\n",
    "    \n",
    "    # LOOCV \n",
    "    loo = LeaveOneOut()\n",
    "\n",
    "    # start model training\n",
    "    result_dict = {}\n",
    "    weight_importance_dict = {}    \n",
    "\n",
    "    for model_name, model in models:        \n",
    "        model_hyperparamsearch = GridSearchCV(estimator=model, param_grid=param_grid[model_name], cv=loo, verbose=0, n_jobs=-1)\n",
    "        model_hyperparamsearch.fit(X_train, y_train)\n",
    "        best_estimator = model_hyperparamsearch.best_estimator_\n",
    "\n",
    "        # get prediction on the test dataset\n",
    "        pred_test = model_hyperparamsearch.predict(X_test)\n",
    "\n",
    "        # keept test performance results\n",
    "        result_dict[model_name] = {}\n",
    "        result_dict[model_name]['uncertainty_mean'] = best_estimator.predict_proba(X_test).max(1).mean()\n",
    "        result_dict[model_name]['uncertainty_std'] = best_estimator.predict_proba(X_test).max(1).std()\n",
    "        result_dict[model_name]['test_acc'] = metrics.accuracy_score(y_test, pred_test)\n",
    "        result_dict[model_name]['test_acc_bal'] = metrics.balanced_accuracy_score(y_test, pred_test)\n",
    "        result_dict[model_name]['test_f1'] = metrics.f1_score(y_test, pred_test, average='binary')\n",
    "        result_dict[model_name]['test_cohen_kappa'] = metrics.cohen_kappa_score(y_test, pred_test, labels=y)\n",
    "        result_dict[model_name]['val'] = model_hyperparamsearch.best_score_\n",
    "        pprint(result_dict)\n",
    "\n",
    "        # plot confusion matrix\n",
    "        cm = metrics.confusion_matrix(y_test, pred_test, labels=y)\n",
    "        cm = cm/float(cm.sum()) # convert to ratio\n",
    "        ax = sns.heatmap(cm, annot=True, xticklabels=y, yticklabels=y, cmap='Blues', fmt='.2f', square=True)\n",
    "        plt.suptitle(f'{model_name}')\n",
    "        plt.tight_layout()\n",
    "        plt.savefig(os.path.join(config_path,'reult_dir_3','fig', 'confusion_matrix', model_name+'.png'))\n",
    "        plt.close()\n",
    "\n",
    "        # plot ROC curve\n",
    "        fpr, tpr, thresholds = roc_curve(y_test,pred_test)\n",
    "        plt.plot(fpr, tpr, 'o-', label=\"Decision Tree\")\n",
    "        plt.plot([0, 1], [0, 1], 'k--', label=\"random guess\")\n",
    "        plt.xlabel('Fall-Out')\n",
    "        plt.ylabel('Recall')\n",
    "        plt.title('Receiver operating characteristic example')\n",
    "        plt.savefig(os.path.join(config_path,'reult_dir_3', 'fig', 'roc', model_name+'.png'))\n",
    "        plt.close()\n",
    "\n",
    "        # save model and results\n",
    "        joblib.dump(best_estimator, os.path.join(config_path,'reult_dir_3', 'model', model_name+'.joblib'))\n",
    "        pd.DataFrame.from_dict(model_hyperparamsearch.cv_results_).to_csv(os.path.join(config_path,'reult_dir_3', 'result', 'cv', model_name+'.csv'))\n",
    "        pd.DataFrame.from_dict(result_dict).T.to_csv(os.path.join(config_path,'reult_dir_3', 'result', 'result.csv'))\n",
    "        pd.DataFrame.from_dict(weight_importance_dict).T.to_csv(os.path.join(config_path,'reult_dir_3', 'result', 'importance_weight.csv'))  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Features [ 25  26  27  43  61  62  63  79  97  98  99 115 133 134 135 151 169 170\n",
      " 171 187 205 206 207 223 241 242 243 259 277 278 279 295 313 314 315 331\n",
      " 349 350 351 367] are constant.\n",
      "invalid value encountered in divide\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'Dummy': {'test_acc': 0.5714285714285714,\n",
      "           'test_acc_bal': 0.5,\n",
      "           'test_cohen_kappa': 0.0,\n",
      "           'test_f1': 0.0,\n",
      "           'uncertainty_mean': 0.5263157894736842,\n",
      "           'uncertainty_std': 0.0,\n",
      "           'val': 0.5263157894736842}}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "3800 fits failed out of a total of 9120.\n",
      "The score on these train-test partitions for these parameters will be set to nan.\n",
      "If these failures are not expected, you can try to debug them by setting error_score='raise'.\n",
      "\n",
      "Below are more details about the failures:\n",
      "--------------------------------------------------------------------------------\n",
      "760 fits failed with the following error:\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\USER\\anaconda3\\envs\\main\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 686, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"c:\\Users\\USER\\anaconda3\\envs\\main\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 1162, in fit\n",
      "    solver = _check_solver(self.solver, self.penalty, self.dual)\n",
      "  File \"c:\\Users\\USER\\anaconda3\\envs\\main\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 54, in _check_solver\n",
      "    raise ValueError(\n",
      "ValueError: Solver newton-cg supports only 'l2' or 'none' penalties, got l1 penalty.\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "760 fits failed with the following error:\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\USER\\anaconda3\\envs\\main\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 686, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"c:\\Users\\USER\\anaconda3\\envs\\main\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 1162, in fit\n",
      "    solver = _check_solver(self.solver, self.penalty, self.dual)\n",
      "  File \"c:\\Users\\USER\\anaconda3\\envs\\main\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 54, in _check_solver\n",
      "    raise ValueError(\n",
      "ValueError: Solver lbfgs supports only 'l2' or 'none' penalties, got l1 penalty.\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "760 fits failed with the following error:\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\USER\\anaconda3\\envs\\main\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 686, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"c:\\Users\\USER\\anaconda3\\envs\\main\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 1162, in fit\n",
      "    solver = _check_solver(self.solver, self.penalty, self.dual)\n",
      "  File \"c:\\Users\\USER\\anaconda3\\envs\\main\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 54, in _check_solver\n",
      "    raise ValueError(\n",
      "ValueError: Solver newton-cg supports only 'l2' or 'none' penalties, got elasticnet penalty.\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "760 fits failed with the following error:\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\USER\\anaconda3\\envs\\main\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 686, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"c:\\Users\\USER\\anaconda3\\envs\\main\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 1162, in fit\n",
      "    solver = _check_solver(self.solver, self.penalty, self.dual)\n",
      "  File \"c:\\Users\\USER\\anaconda3\\envs\\main\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 54, in _check_solver\n",
      "    raise ValueError(\n",
      "ValueError: Solver lbfgs supports only 'l2' or 'none' penalties, got elasticnet penalty.\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "760 fits failed with the following error:\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\USER\\anaconda3\\envs\\main\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 686, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"c:\\Users\\USER\\anaconda3\\envs\\main\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 1162, in fit\n",
      "    solver = _check_solver(self.solver, self.penalty, self.dual)\n",
      "  File \"c:\\Users\\USER\\anaconda3\\envs\\main\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 64, in _check_solver\n",
      "    raise ValueError(\n",
      "ValueError: Only 'saga' solver supports elasticnet penalty, got solver=liblinear.\n",
      "\n",
      "One or more of the test scores are non-finite: [       nan        nan 0.52631579 0.47368421 0.47368421 0.47368421\n",
      " 0.94736842 0.31578947        nan        nan        nan 0.26315789\n",
      "        nan        nan 0.52631579 0.26315789 0.47368421 0.47368421\n",
      " 0.94736842 0.36842105        nan        nan        nan 0.36842105\n",
      "        nan        nan 0.52631579 0.47368421 0.47368421 0.47368421\n",
      " 0.94736842 0.36842105        nan        nan        nan 0.31578947\n",
      "        nan        nan 0.52631579 0.63157895 0.47368421 0.47368421\n",
      " 0.94736842 0.31578947        nan        nan        nan 0.36842105\n",
      "        nan        nan 0.52631579 0.52631579 0.47368421 0.47368421\n",
      " 0.94736842 0.26315789        nan        nan        nan 0.52631579\n",
      "        nan        nan 0.52631579 0.57894737 0.47368421 0.47368421\n",
      " 0.94736842 0.36842105        nan        nan        nan 0.42105263\n",
      "        nan        nan 0.52631579 0.31578947 0.47368421 0.47368421\n",
      " 0.94736842 0.36842105        nan        nan        nan 0.36842105\n",
      "        nan        nan 0.52631579 0.36842105 0.47368421 0.47368421\n",
      " 0.94736842 0.47368421        nan        nan        nan 0.42105263\n",
      "        nan        nan 0.52631579 0.52631579 0.47368421 0.47368421\n",
      " 0.94736842 0.36842105        nan        nan        nan 0.52631579\n",
      "        nan        nan 0.52631579 0.42105263 0.47368421 0.47368421\n",
      " 0.94736842 0.42105263        nan        nan        nan 0.31578947\n",
      "        nan        nan 0.52631579 0.21052632 0.52631579 0.52631579\n",
      " 0.94736842 0.52631579        nan        nan        nan 0.26315789\n",
      "        nan        nan 0.52631579 0.47368421 0.52631579 0.52631579\n",
      " 0.94736842 0.52631579        nan        nan        nan 0.57894737\n",
      "        nan        nan 0.52631579 0.26315789 0.52631579 0.52631579\n",
      " 0.94736842 0.52631579        nan        nan        nan 0.31578947\n",
      "        nan        nan 0.52631579 0.26315789 0.52631579 0.52631579\n",
      " 0.94736842 0.52631579        nan        nan        nan 0.36842105\n",
      "        nan        nan 0.52631579 0.52631579 0.52631579 0.52631579\n",
      " 0.94736842 0.52631579        nan        nan        nan 0.10526316\n",
      "        nan        nan 0.52631579 0.21052632 0.84210526 0.84210526\n",
      " 0.94736842 0.84210526        nan        nan        nan 0.26315789\n",
      "        nan        nan 0.52631579 0.36842105 0.84210526 0.84210526\n",
      " 0.94736842 0.84210526        nan        nan        nan 0.36842105\n",
      "        nan        nan 0.52631579 0.21052632 0.84210526 0.84210526\n",
      " 0.94736842 0.84210526        nan        nan        nan 0.42105263\n",
      "        nan        nan 0.52631579 0.42105263 0.84210526 0.84210526\n",
      " 0.94736842 0.84210526        nan        nan        nan 0.84210526\n",
      "        nan        nan 0.52631579 0.26315789 0.84210526 0.84210526\n",
      " 0.94736842 0.84210526        nan        nan        nan 0.84210526\n",
      "        nan        nan 0.89473684 0.89473684 0.94736842 0.94736842\n",
      " 0.94736842 0.94736842        nan        nan        nan 0.89473684\n",
      "        nan        nan 0.89473684 0.89473684 0.94736842 0.94736842\n",
      " 0.94736842 0.94736842        nan        nan        nan 0.89473684\n",
      "        nan        nan 0.89473684 0.89473684 0.94736842 0.94736842\n",
      " 0.94736842 0.94736842        nan        nan        nan 0.89473684\n",
      "        nan        nan 0.89473684 0.89473684 0.94736842 0.94736842\n",
      " 0.94736842 0.94736842        nan        nan        nan 0.94736842\n",
      "        nan        nan 0.89473684 0.89473684 0.94736842 0.94736842\n",
      " 0.94736842 0.94736842        nan        nan        nan 0.94736842\n",
      "        nan        nan 0.94736842 0.94736842 0.89473684 0.89473684\n",
      " 1.         0.89473684        nan        nan        nan 0.89473684\n",
      "        nan        nan 0.94736842 0.94736842 0.89473684 0.89473684\n",
      " 1.         0.89473684        nan        nan        nan 0.89473684\n",
      "        nan        nan 0.94736842 0.94736842 0.89473684 0.89473684\n",
      " 1.         0.89473684        nan        nan        nan 0.89473684\n",
      "        nan        nan 0.94736842 0.94736842 0.89473684 0.89473684\n",
      " 1.         0.89473684        nan        nan        nan 0.89473684\n",
      "        nan        nan 0.94736842 0.94736842 0.89473684 0.89473684\n",
      " 1.         0.89473684        nan        nan        nan 0.89473684\n",
      "        nan        nan 0.89473684 0.89473684 0.89473684 0.89473684\n",
      " 0.89473684 0.89473684        nan        nan        nan 0.89473684\n",
      "        nan        nan 0.89473684 0.89473684 0.89473684 0.89473684\n",
      " 0.89473684 0.89473684        nan        nan        nan 0.89473684\n",
      "        nan        nan 0.89473684 0.89473684 0.89473684 0.89473684\n",
      " 0.89473684 0.89473684        nan        nan        nan 0.89473684\n",
      "        nan        nan 0.89473684 0.89473684 0.89473684 0.89473684\n",
      " 0.89473684 0.89473684        nan        nan        nan 0.89473684\n",
      "        nan        nan 0.89473684 0.89473684 0.89473684 0.89473684\n",
      " 0.89473684 0.89473684        nan        nan        nan 0.89473684\n",
      "        nan        nan 0.89473684 0.89473684 0.89473684 0.89473684\n",
      " 0.89473684 0.89473684        nan        nan        nan 0.89473684\n",
      "        nan        nan 0.94736842 0.89473684 0.89473684 0.89473684\n",
      " 0.89473684 0.89473684        nan        nan        nan 0.89473684\n",
      "        nan        nan 0.94736842 0.89473684 0.89473684 0.89473684\n",
      " 0.89473684 0.89473684        nan        nan        nan 0.89473684\n",
      "        nan        nan 0.94736842 0.89473684 0.89473684 0.89473684\n",
      " 0.89473684 0.89473684        nan        nan        nan 0.89473684\n",
      "        nan        nan 0.94736842 0.89473684 0.89473684 0.89473684\n",
      " 0.89473684 0.89473684        nan        nan        nan 0.89473684]\n",
      "l1_ratio parameter is only used when penalty is 'elasticnet'. Got (penalty=l2)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'Dummy': {'test_acc': 0.5714285714285714,\n",
      "           'test_acc_bal': 0.5,\n",
      "           'test_cohen_kappa': 0.0,\n",
      "           'test_f1': 0.0,\n",
      "           'uncertainty_mean': 0.5263157894736842,\n",
      "           'uncertainty_std': 0.0,\n",
      "           'val': 0.5263157894736842},\n",
      " 'LogReg': {'test_acc': 0.7142857142857143,\n",
      "            'test_acc_bal': 0.7083333333333333,\n",
      "            'test_cohen_kappa': 0.41666666666666663,\n",
      "            'test_f1': 0.6666666666666666,\n",
      "            'uncertainty_mean': 0.9239265084415076,\n",
      "            'uncertainty_std': 0.05589013696041133,\n",
      "            'val': 1.0}}\n",
      "{'Dummy': {'test_acc': 0.5714285714285714,\n",
      "           'test_acc_bal': 0.5,\n",
      "           'test_cohen_kappa': 0.0,\n",
      "           'test_f1': 0.0,\n",
      "           'uncertainty_mean': 0.5263157894736842,\n",
      "           'uncertainty_std': 0.0,\n",
      "           'val': 0.5263157894736842},\n",
      " 'LogReg': {'test_acc': 0.7142857142857143,\n",
      "            'test_acc_bal': 0.7083333333333333,\n",
      "            'test_cohen_kappa': 0.41666666666666663,\n",
      "            'test_f1': 0.6666666666666666,\n",
      "            'uncertainty_mean': 0.9239265084415076,\n",
      "            'uncertainty_std': 0.05589013696041133,\n",
      "            'val': 1.0},\n",
      " 'SVM': {'test_acc': 0.8571428571428571,\n",
      "         'test_acc_bal': 0.8333333333333333,\n",
      "         'test_cohen_kappa': 0.6956521739130435,\n",
      "         'test_f1': 0.8,\n",
      "         'uncertainty_mean': 0.7217816293420878,\n",
      "         'uncertainty_std': 0.11380497296576783,\n",
      "         'val': 1.0}}\n",
      "{'Dummy': {'test_acc': 0.5714285714285714,\n",
      "           'test_acc_bal': 0.5,\n",
      "           'test_cohen_kappa': 0.0,\n",
      "           'test_f1': 0.0,\n",
      "           'uncertainty_mean': 0.5263157894736842,\n",
      "           'uncertainty_std': 0.0,\n",
      "           'val': 0.5263157894736842},\n",
      " 'LogReg': {'test_acc': 0.7142857142857143,\n",
      "            'test_acc_bal': 0.7083333333333333,\n",
      "            'test_cohen_kappa': 0.41666666666666663,\n",
      "            'test_f1': 0.6666666666666666,\n",
      "            'uncertainty_mean': 0.9239265084415076,\n",
      "            'uncertainty_std': 0.05589013696041133,\n",
      "            'val': 1.0},\n",
      " 'RandomForest': {'test_acc': 0.8571428571428571,\n",
      "                  'test_acc_bal': 0.8333333333333333,\n",
      "                  'test_cohen_kappa': 0.6956521739130435,\n",
      "                  'test_f1': 0.8,\n",
      "                  'uncertainty_mean': 0.6726190476190477,\n",
      "                  'uncertainty_std': 0.13442368798376447,\n",
      "                  'val': 0.8947368421052632},\n",
      " 'SVM': {'test_acc': 0.8571428571428571,\n",
      "         'test_acc_bal': 0.8333333333333333,\n",
      "         'test_cohen_kappa': 0.6956521739130435,\n",
      "         'test_f1': 0.8,\n",
      "         'uncertainty_mean': 0.7217816293420878,\n",
      "         'uncertainty_std': 0.11380497296576783,\n",
      "         'val': 1.0}}\n",
      "{'Dummy': {'test_acc': 0.5714285714285714,\n",
      "           'test_acc_bal': 0.5,\n",
      "           'test_cohen_kappa': 0.0,\n",
      "           'test_f1': 0.0,\n",
      "           'uncertainty_mean': 0.5263157894736842,\n",
      "           'uncertainty_std': 0.0,\n",
      "           'val': 0.5263157894736842},\n",
      " 'LogReg': {'test_acc': 0.7142857142857143,\n",
      "            'test_acc_bal': 0.7083333333333333,\n",
      "            'test_cohen_kappa': 0.41666666666666663,\n",
      "            'test_f1': 0.6666666666666666,\n",
      "            'uncertainty_mean': 0.9239265084415076,\n",
      "            'uncertainty_std': 0.05589013696041133,\n",
      "            'val': 1.0},\n",
      " 'MLP': {'test_acc': 0.7142857142857143,\n",
      "         'test_acc_bal': 0.7083333333333333,\n",
      "         'test_cohen_kappa': 0.41666666666666663,\n",
      "         'test_f1': 0.6666666666666666,\n",
      "         'uncertainty_mean': 0.88731275047856,\n",
      "         'uncertainty_std': 0.14159679003149725,\n",
      "         'val': 1.0},\n",
      " 'RandomForest': {'test_acc': 0.8571428571428571,\n",
      "                  'test_acc_bal': 0.8333333333333333,\n",
      "                  'test_cohen_kappa': 0.6956521739130435,\n",
      "                  'test_f1': 0.8,\n",
      "                  'uncertainty_mean': 0.6726190476190477,\n",
      "                  'uncertainty_std': 0.13442368798376447,\n",
      "                  'val': 0.8947368421052632},\n",
      " 'SVM': {'test_acc': 0.8571428571428571,\n",
      "         'test_acc_bal': 0.8333333333333333,\n",
      "         'test_cohen_kappa': 0.6956521739130435,\n",
      "         'test_f1': 0.8,\n",
      "         'uncertainty_mean': 0.7217816293420878,\n",
      "         'uncertainty_std': 0.11380497296576783,\n",
      "         'val': 1.0}}\n"
     ]
    }
   ],
   "source": [
    "main(config_path,target)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "main",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.15 (main, Nov 24 2022, 14:39:17) [MSC v.1916 64 bit (AMD64)]"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "eeee73005f2d7c442ce01edc53b42ba7e639fc43199e10e3461fac3e3675672b"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
